%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2023}

\begin{document}

\twocolumn[
	\icmltitle{An Efficient and Memory-Safe Implementation of the Alternating Least Square Recommender System Algorithm}

	% It is OKAY to include author information, even for blind
	% submissions: the style file will automatically remove it for you
	% unless you've provided the [accepted] option to the icml2023
	% package.

	% List of affiliations: The first argument should be a (short)
	% identifier you will use later to specify author affiliations
	% Academic affiliations should list Department, University, City, Region, Country
	% Industry affiliations should list Company, City, Region, Country

	% You can specify symbols, otherwise they are numbered in order.
	% Ideally, you should not use this facility. Affiliations will be numbered
	% in order of appearance and this is the preferred way.
	% \icmlsetsymbol{equal}{*}

	\begin{icmlauthorlist}
		\icmlauthor{Abdulrasheed Bolaji Fawole}{aims}
	\end{icmlauthorlist}

	\icmlaffiliation{aims}{African Institute for Mathematical Sciences (AIMS) South Africa, 6 Melrose Road, Muizenberg 7975, Cape Town, South Africa}
	\icmlcorrespondingauthor{Ulrich Paquet}{ulrich@aims.ac.za}

	% You may provide any keywords that you
	% find helpful for describing your paper; these are used to populate
	% the "keywords" metadata in the PDF but will not be shown in the document
	\icmlkeywords{Collaborative Filtering, Matrix Factorization, Alternating Least Squares, Memory Efficiency, Recommender Systems, Large-Scale Machine Learning}

	\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Alternating Least Squares (ALS) remains a widely used optimization method for matrix factorization–based recommender systems, yet practical deployments are often constrained by memory usage and data ingestion bottlenecks on large, sparse datasets.
This paper presents an efficient and memory-safe implementation of ALS that combines sparse matrix representations, chunked two-pass data loading, and low-level memory management techniques to enable scalable training on datasets that exceed main memory capacity.
Using the MovieLens 100K and 32M benchmarks, we demonstrate how complementary use of CSR and CSC formats, float32 precision, and vectorized computations substantially reduce memory footprint while preserving predictive performance.
We evaluate both bias-only and bias-latent ALS variants, showing stable convergence and competitive RMSE at scale.
The proposed implementation provides a practical alternative to heavyweight distributed frameworks, making large-scale collaborative filtering accessible on modest hardware.

The source code for this system can be found at \url{https://https://github.com/Abdulrasheed1729/rekomenda}
\end{abstract}

\section{Introduction}
Recommender systems have become a salient part of modern e-commerce, offering unprecedented opportunities to meet a variety of special needs and tastes as consumers are inundated with vast selections of products offered by electronic retailers and content providers.
These systems analyze patterns of user interest in products to provide personalized recommendations that enhance user satisfaction and loyalty.

These systems are particularly effective for entertainment products, such as movies, music, and TV shows, where large volumes of data regarding customer satisfaction are available.
Furthermore, recommender systems serve a critical function in marketplaces by addressing the power-law distribution of sales, ensuring that suitable, less popular "tail items" are exposed to users, thereby increasing user engagement.

Over the years many stragies have been employed and they can all be categorised into two main broad categories:
\begin{itemize}
	\item \textbf{Content Filtering}: This approach characterizes users and products by creating explicit profiles based on attributes, such as movie genre, actors, or user demographics.
	      The profiles allow programs to associate users with matching products.
	      A known successful realization of this strategy is the Music Genome Project utilized by Pandora.com \cite{castelluccio2006music}, where trained music analysts score each song based on hundreds of distinct musical characteristics, or "genes," relevant to understanding listeners’ preferences. However, content-based strategies require external information that might not be available or easy to collect. Content filtering is superior when dealing with the cold start problem for new products and users.
	\item \textbf{Collaborative Filtering}: This alternative relies solely on past user behavior, such as previous transactions or product ratings, without requiring the creation of explicit profiles.
	      CF analyzes relationships between users and interdependencies among products to identify new user-item associations.
	      This approach is domain free and often achieves greater accuracy than content-based techniques, though it suffers from the cold start problem for new users and items.
	      The term "collaborative filtering" was coined by the developers of Tapestry\cite{goldberg1992using}, the first recommender system.

	      Within collaborative filtering, techniques are primarily grouped into \emph{neighborhood methods} and \emph{latent factor} models. Latent factor models explain ratings by characterizing both items and users on 20 to 100 factors inferred from the rating patterns, acting as a computerized alternative to human-created characteristics like song genres. The resulting interaction between a user and an item captures the user’s overall interest in the item’s characteristics.

	      The matrix factorisation technique which is approach in collaborative filtering, map both users and items to a \emph{joint latent space}.
	      This approach rose to prominence due to its demonstrable superiority in predictive accuracy over classic nearest-neighbor techniques. The Netflix Prize competition, which released a training set of over 100 million ratings, dramatically spurred research in this area and highlighted the potential of MF models\cite{koren2009matrix}. Furthermore, MF techniques are attractive because they offer a compact, memory-efficient model, good scalability, and significant flexibility for modeling complex aspects of real-life data.
\end{itemize}

\section{Related Work}

The evolution of recommender systems has been characterized by a paradigm shift from heuristic neighborhood-based methods to sophisticated latent factor architectures capable of modeling complex user-item interactions \cite{koren2009matrix, pires2022interact2vec}.
Early collaborative filtering (CF) relied on finding similar neighbors to generate recommendations, but these approaches struggled with the inherent sparsity and scalability challenges of modern industrial datasets \cite{koren2009matrix, pires2022interact2vec}.


\subsection{Latent Factor Models and Matrix Factorization}
Latent factor models, particularly matrix factorization (MF), emerged as a dominant framework by mapping both users and items into a shared $k$-dimensional latent space \cite{koren2009matrix}.
In this space, the affinity between a user $u$ and an item $i$ is represented as the inner product of their respective latent vectors, $p_u$ and $q_i$ \cite{koren2009matrix}.
To prevent overfitting in sparse environments, modern MF models optimize a regularized squared error objective function, which penalizes the magnitudes of the learned parameters \cite{perez14, koren2009matrix}.
Predictive accuracy is further enhanced by incorporating baseline biases that account for systematic variations independent of user-item interactions, such as a user's general rating scale or an item's universal popularity \cite{koren2009matrix}.


\subsection{Optimization and Distributed Scaling}
Efficiency in learning these latent structures is critical for large-scale applications.
Stochastic Gradient Descent (SGD) is widely utilized for its simplicity and ability to perform incremental updates in streaming data environments \cite{koren2009matrix, perez14}.
Conversely, Alternating Least Squares (ALS) is often preferred for implicit feedback or for its natural parallelization properties \cite{perez14, hu2008collaborative}.
Distributed implementations of ALS typically employ Join, Broadcast, or Block strategies to minimize communication overhead across clusters, with Block ALS proving most effective for minimizing network traffic in massive-scale deployments \cite{perez14}.


\subsection{Temporal Dynamics and Preference Drift}
A significant limitation of static CF models is their inability to account for ``concept drift,'' where user preferences and item perceptions evolve over time \cite{koren2009collaborative, hu2008collaborative}.
To address this, the \texttt{timeSVD++} model introduces time-dependent components into the bias terms and latent factor vectors \cite{koren2009collaborative, hu2008collaborative}.
This is often modeled using a time deviation function, $dev_u(t)=sign(t-t_u)\cdot|t-t_u|^\beta$, which tracks the distance from a user's mean rating date \cite{koren2009collaborative}.
Additionally, researchers have demonstrated that weighting recent interactions more heavily---effectively ``aging'' older data---can significantly improve predictive accuracy (RMSE) and better reflect a user's current interests \cite{bakir2018collaborative}.


\subsection{Implicit Feedback and Bayesian Inference}
In many real-world scenarios, systems must rely on implicit signals (e.g., viewing history, clicks) rather than explicit ratings \cite{stern2009matchbox, hu2008collaborative}.
These one-class datasets are often modeled using confidence-weighted frameworks that treat interactions as binary preference indicators with varying degrees of certainty \cite{koren2009matrix, hu2008collaborative}.
Bayesian generative models, such as the Matchbox system, extend this by mapping users and items into a trait space where inference is performed via approximate message passing techniques like Expectation Propagation (EP) and Variational Message Passing (VMP) \cite{stern2009matchbox}.
This approach allows for the seamless integration of user and item metadata while supporting online, single-pass updates through Assumed-Density Filtering (ADF) \cite{stern2009matchbox}.

\subsection{Structural Context and Taxonomies}
Advanced recommenders also leverage the structural relationships between items to mitigate extreme sparsity.
In the music domain, for instance, items are organized into a four-level taxonomy comprising tracks, albums, artists, and genres \cite{koenigstein2011yahoo}.
Models utilizing these hierarchies can infer preferences for new tracks based on a user's historical affinity for parent artists or genres, providing a robust solution to the ``cold-start'' problem \cite{koenigstein2011yahoo}.
Recent developments have integrated these structural insights with deep learning architectures, such as self-attention mechanisms and recurrent units, to capture the sequential nature of user behavior \cite{koren2009collaborative, pires2022interact2vec}.

\subsection{Scalable Implementations}

Apache Spark MLlib provides a distributed implementation of ALS that scales to billions of ratings \cite{meng2016mllib, zaharia2016apache}.
While effective for massive datasets, this approach requires substantial infrastructure and operational expertise.
\cite{zhou2008large} proposed block-based updates for improved cache locality.
Takács and Tikk \cite{takacs2012alternating} introduced alternating kernel-based matrix factorization with competitive performance on standard benchmarks.


More recent work has explored neural collaborative filtering \cite{he2017neural}, graph neural networks for recommendations \cite{fan2019graph}, and transformer-based approaches \cite{sun2019bert4rec}.
However, these methods typically require significantly more computational resources than traditional matrix factorization while providing marginal improvements for many practical applications \cite{rendle2019difficulty}.

\subsection{Memory-Efficient Learning}

Streaming algorithms for large-scale machine learning have been extensively studied \cite{dyer2013greedy, liberty2013simple}.
% Feldman et al. [20] analyzed coresets for matrix factorization problems.
Online matrix factorization approaches update models incrementally as new data arrives.
Our work differs by focusing on efficient batch processing of static datasets through intelligent data loading strategies rather than algorithmic modifications.


External memory algorithms and out-of-core computing provide general frameworks for data larger than RAM.
Our chunked loading approach can be viewed as a specialized application of these principles to the ALS training pipeline.

\section{Methodology}

\subsection{Problem Formulation}

The main aim in the alternating least square algorithm is to minimise the following log-likelihood objective function.

\begin{equation}
	\label{eqn:01}
	\log p(R \mid U, V) = - \frac{\lambda}{2} \sum_{m = 1}^{M} \sum_{n \in \Omega(m)} (r_{mn} - u_m^T v_n)^2
\end{equation}

Some regularisation, like bias, latent vectors, etc, can be introduced to improve the algorithm. We'll see this in the result for different instances in our model.

\subsection{Data Indexing and Representation}

To carry out the assignment in this work, we used the MovieLens 32M and 100K datasets \cite{harper2015movielens}.
As it is expected with any data involving user interaction, there is always a level of sparsity.
In the sense that users have diverse choices and this contributes alot to the sparsity of the datasets.
For example in the 32M dataset there are 87,585 movies and 200,948 users \cite{harper2015movielens}, which make the sparsity of the dataset obvious.
As an example if we consider following table:

\begin{table}[H]
	\centering
	\label{tab:01}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{User ID} & \textbf{Movie ID} & \textbf{Rating} \\
		\hline
		Alice            & Titanic           & 5.0             \\
		\hline
		Bob              & Avatar            & 4.0             \\
		\hline
		Alice            & Avatar            & 3.0             \\
		\hline
		Charlie          & Titanic           & 4.5             \\
		\hline
		Alice            & Mr Bean           & 3.0             \\
		\hline
		Caleb            & Titanic           & 3.5             \\
		\hline
	\end{tabular}
	\caption{Example Movie Ratings Data}
\end{table}

In the above table there are 4 users and 3 movies. Represent table \ref{tab:01} as a matrix we have the following sparse matrix:

\begin{table}[H]
	\centering
	\label{tab:02}
	\begin{tabular}{cccc}
		        & Titanic & Avatar & Mr. Bean \\
		Alice   & 5.0     & 3.0    & 3.0      \\
		Bob     & -       & 4.0    & -        \\
		Charlie & 4.5     & -      & -        \\
		Caleb   & 3.5     & -      & -        \\
	\end{tabular}
\end{table}

From the above matrix it obvious that only 6 entries out of 12 is useful for analysis. In a very large dataset with say $n$ number of users and $m$ numbers of items, the dimension of the sparse matrix is $n \times m$, which will result in a space complexity of $nm$. But this can substantially reduced by storing only the non-zero values.



\begin{figure}[H]
	\centerline{\includegraphics[width=\linewidth]{pics/degree_distribution_loglog_simple.pdf}}
	% \caption{}
\end{figure}

\begin{figure}[H]
	\centerline{\includegraphics[width=\linewidth]{pics/ratings_distribution_simple.pdf}}
	\caption{The ratings distribution of the MovieLens 32M dataset}
\end{figure}

\subsubsection{The Compressed Sparse Row (CSR)}

This is a sparse matrix represented by the following parameters:

\begin{itemize}
	\item The number of rows in the matrix.

	\item The number of columns in the matrix.

	\item The number of non-zero elements (nnz) in the matrix.

	\item The pointers to the row offsets array of length number of rows + 1 that represents the starting position of each row in the columns and values arrays.

	\item The pointers to the column indices array of length nnz that contains the column indices of the corresponding elements in the values array.

	\item The pointers to the values array of length nnz that holds all nonzero values of the matrix in row-major ordering.
\end{itemize}

\begin{figure}[H]
	\centerline{\includegraphics[width=\linewidth]{pics/csr.png}}
	\caption{A visual representation of the CSR format \cite{NVIDIAsparse}:w}
\end{figure}

\subsubsection{The Coordinate Format (COO)}

This is a sparse matrix represented by the following parameters:

\begin{itemize}
	\item The number of rows in the matrix.

	\item The number of columns in the matrix.

	\item The number of non-zero elements (nnz) in the matrix.

	\item The pointers to the row indices array of length nnz that contains the row indices of the corresponding elements in the values array.

	\item The pointers to the column indices array of length nnz that contains the column indices of the corresponding elements in the values array.

	\item The pointers to the values array of length nnz that holds all nonzero values of the matrix in row-major ordering.

	\item Each entry of the COO representation consists of a (row, column) pair.

	\item The COO format is assumed to be sorted by row.
\end{itemize}

\begin{figure}[H]
	\centerline{\includegraphics[width=\linewidth]{pics/coo.png}}
	\caption{A visual representation of the COO format \cite{NVIDIAsparse}:w}
\end{figure}

\subsubsection{The Compresses Sparse Column (CSC)}

This is a sparse matrix represented by the following parameters:

\begin{itemize}
	\item The number of rows in the matrix.

	\item The number of columns in the matrix.

	\item The number of non-zero elements (nnz) in the matrix.

	\item The pointers to the column offsets array of length number of column + 1 that represents the starting position of each column in the columns and values arrays.

	\item The pointers to the row indices array of length nnz that contains row indices of the corresponding elements in the values array.

	\item The pointers to the values array of length nnz that holds all nonzero values of the matrix in column-major ordering.
\end{itemize}

\begin{figure}[H]
	\centerline{\includegraphics[width=\linewidth]{pics/csc.png}}
	\caption{A visual representation of the CSC format \cite{NVIDIAsparse}:w}
\end{figure}

In order to be efficient in the implementation of our, the different sparse matrix representations was used in a complementary manner. The CSR representation was used

\subsection{Data Ingestion and Preprocessing}

Given the size of the dataset, chunking technique was employed to make the ingestion of data memory-efficient, and we used a two pass streaming approach to achieve this:

\begin{enumerate}
	\item \textbf{First Pass}: iteratively processes the ratings file in chunk to collect the set of unique user and item identifiers and to count the total number of ratings. This enables mapping original IDs to contiguous integer indices without loading the entire dataset into memory.

	\item \textbf{Second Pass}: reprocesses the ratings file in chunks to map original IDs to contiguous indices and performs a randomized train/test split. The split uses a per-chunk random sampling controlled by a \texttt{random\_seed} for reproducibility.

	      % \item \textbf{Output}: training data is returned as sparse matrices in both CSR and CSC formats; test data is retained as dense arrays of (user\_index, item\_index, rating). The function also returns mappings from internal item indices to movie titles.
\end{enumerate}


In addition, even though Python is a garbage-collected language, we need to employed the \texttt{malloc - free} memory management model from the \texttt{C} programming language \cite{kernighan1988c}, this because we notice our program crashout after two chunking process.
We delete any unused data object from the memory using the \texttt{del} keyword in Python.
The two-pass is as shown in algorithm (\ref{alg:sparse_matrix_construction})

\begin{algorithm}[t]
	\caption{Two-Pass Chunked Sparse Matrix Construction}
	\label{alg:sparse_matrix_construction}
	\begin{algorithmic}[1]
		\REQUIRE Ratings file path $\mathcal{F}$, chunk size $c$
		\ENSURE Sparse matrices $\mathbf{M}_{\text{CSR}}$, $\mathbf{M}_{\text{CSC}}$, test arrays $\mathcal{T}$

		\STATE \textbf{Pass 1: Collect Unique Identifiers}
		\STATE Initialize: $\mathcal{U}_{\text{users}} \leftarrow \emptyset$, $\mathcal{U}_{\text{items}} \leftarrow \emptyset$
		\FOR{each chunk $\mathcal{C}$ of $c$ rows from $\mathcal{F}$}
		\STATE Parse $\{\text{user\_id}_i\}$, $\{\text{item\_id}_i\}$ from $\mathcal{C}$
		\STATE $\mathcal{U}_{\text{users}} \leftarrow \mathcal{U}_{\text{users}} \cup \{\text{user\_id}_i\}$
		\STATE $\mathcal{U}_{\text{items}} \leftarrow \mathcal{U}_{\text{items}} \cup \{\text{item\_id}_i\}$
		\STATE Discard $\mathcal{C}$
		\ENDFOR
		\STATE Create mappings: $\phi_u: \text{user\_id} \mapsto \text{idx}$, $\phi_v: \text{item\_id} \mapsto \text{idx}$

		\STATE
		\STATE \textbf{Pass 2: Load and Split}
		\STATE Initialize: $\mathcal{L}_{\text{train}} \leftarrow []$, $\mathcal{L}_{\text{test}} \leftarrow []$
		\FOR{each chunk $\mathcal{C}$ of $c$ rows from $\mathcal{F}$}
		\STATE Parse $\{\text{user\_id}_i\}$, $\{\text{item\_id}_i\}$, $\{r_i\}$ from $\mathcal{C}$
		\STATE Map IDs to indices using $\phi_u$, $\phi_v$
		\STATE Randomly split into train/test with ratio $0.8/0.2$
		\STATE Append train samples to $\mathcal{L}_{\text{train}}$
		\STATE Append test samples to $\mathcal{L}_{\text{test}}$
		\STATE Discard $\mathcal{C}$
		\ENDFOR

		\STATE Concatenate $\mathcal{L}_{\text{train}} \rightarrow \mathcal{A}_{\text{train}}$, $\mathcal{L}_{\text{test}} \rightarrow \mathcal{T}$
		\STATE Convert $\mathcal{A}_{\text{train}} \rightarrow \mathbf{M}_{\text{CSR}}$, $\mathbf{M}_{\text{CSC}}$
		\RETURN $\mathbf{M}_{\text{CSR}}$, $\mathbf{M}_{\text{CSC}}$, $\mathcal{T}$
	\end{algorithmic}
\end{algorithm}


\subsection{Optimisation Techniques}

\begin{enumerate}
\item \textbf{Float32 pecision}: We use float32 instead of float64 for all computations, reducing memory consumption by 50\% with negligible impact on convergence. For 200K users and 50K items with 50 factors.

\begin{itemize}
\item float64: $2 \times (200K \times 50 + 50K \times 50) \times 8 bytes = 100 MB$
\item float32: $2 \times (200K \times 50 + 50K \times 50) \times 4 bytes = 50 MB$
\end{itemize}
\item  \textbf{Vectorized Predictions}: We replace explicit loops with NumPy broadcasting for prediction:

\begin{verbatim}
# Naive:
for idx, (u, i)
    in enumerate(zip(users, items)):
    pred[idx] = μ + b_u[u]
        + b_i[i] +
        u_factors[u] @ i_factors[i]

# Optimized: Single vectorized operation
pred = \mu + b_u[users]
        + b_i[items] +
       sum(u_factors[users] * i_factors[items], axis=1)
\end{verbatim}

\item \textbf{Cached User Indices}: We pre-compute and cache the reconstruction of user indices from CSR \texttt{indptr}:
\begin{verbatim}
user_indices = repeat(arange(m), diff(indptr))
\end{verbatim}
This eliminates redundant computation in each iteration, saving O(m) operations per epoch.

\item \textbf{Early Stopping:} We monitor RMSE convergence and terminate when improvement falls below threshold $\varepsilon$:
\begin{verbatim}
if |RMSE_{t} - RMSE_{t-1}| < epsilon: break
\end{verbatim}
\end{enumerate}




\section{Result}

Now we present different results from our different models.

\subsection{Bias-Only ALS}
The bias-only als as shown in the algorithm(\ref{alg:bias-only-als}). If we add the biases for both user and movie items in equation (\ref{eqn:01}), we have the following resulting equation:
\begin{equation}
\label{eqn:02}
\mathcal{L}  =  - \frac{\lambda}{2} \sum_{mn} (r_{mn} - (b_{m}^{(u)} + b_{n}^{(i)}))^2  - \frac{\gamma}{2} b_{m}^{(u)2} + \ldots
\end{equation}

\begin{figure}[H]
	\centerline{\includegraphics[width=\linewidth]{../notebooks/pics/als-bias-only-train-test.pdf}}
	\caption{RMSE and Negative Log-Likelihood for the MovieLens 100K dataset with the Bias-Only ALS}
\end{figure}

\subsection{Bias-Latent ALS}
In this case latent vectors $U, V$ are introduced for users and movies respectively, and the equation (\ref{eqn:01}) becomes
\begin{align}
\label{eqn:03}
\mathcal{L} &  =  - \frac{\lambda}{2} \sum_{mn} (r_{mn} - (u^T_m v_n + b_{m}^{(u)} + b_{n}^{(i)}))^2 \nonumber \\
& \qquad - \frac{\tau}{2}\sum_{m} u_m^T u_m - \frac{\tau}{2}\sum_{m} v_n^T v_n  + \ldots
\end{align}

Taking the derivative of $\mathcal{L}$ with respect to $u_m$, we get the representation for user update, and this is used in algorithm (\ref{alg:latent-factor-als})

\[
u_m = \bigg( \lambda \sum_n v_n v_n^T + \lambda \mathbb{I} \bigg)^{-1} \bigg( \lambda \sum_n v_n(r_{mn} - b_{m}^{(u)} - b_{n}^{(i)}) \bigg)
\]

In the same manner, the bias update for bot the user and movie are given as follows:

\begin{align*}
b_{m}^{(u)} & = \frac{\lambda\sum_n(r_{mn} - (u_m^Tv_n + b_n^{(i)}))}{\lambda|\Omega(m)| + \gamma} \\
b_{n}^{(i)} & = \frac{\lambda\sum_m(r_{mn} - (u_m^Tv_n + b_m^{(u)}))}{\lambda|\Pi(m)| + \gamma}
\end{align*}

\begin{figure}[H]
	\centerline{\includegraphics[width=\linewidth]{../notebooks/pics/als-bias-latent-train-test-32m.pdf}}
	\caption{RMSE and Negative Log-Likelihood for the MovieLens 32M dataset with the Bias-Latent ALS}
\end{figure}

\begin{figure}[H]
\label{enbed-plot}
\centerline{\includegraphics[width=\linewidth]{../notebooks/pics/embedding-plot.pdf}}
	\caption{2D Embedding Plot for 100 Movies}
\end{figure}
From the plot in figure (\ref{enbed-plot}) we can noticed that movies of like genres are clusteredtogether. For example the movie, \emph{Fond Kiss}, which is a romantic movie is closely surrounded by \emph{Princess Diaries, High Security Vacation}, which are also romantic movies.


\begin{algorithm}[t]
	\caption{Bias-Only Alternating Least Squares}
	\label{alg:bias-only-als}
	\begin{algorithmic}[1]
		\REQUIRE Rating matrix $R \in \mathbb{R}^{m \times n}$ with observed entries $\Omega$, regularization $\lambda$, iterations $T$
		\ENSURE User biases $\{b_u\}_{u=1}^m$, item biases $\{b_i\}_{i=1}^n$

		\STATE \textbf{Initialize:}
		\STATE $\mu \leftarrow \frac{1}{|\Omega|} \sum_{(u,i) \in \Omega} r_{ui}$
		\STATE $b_u \leftarrow 0$ for all $u \in [m]$
		\STATE $b_i \leftarrow 0$ for all $i \in [n]$

		\FOR{$t = 1$ to $T$}
		\STATE \textit{// Update user biases (fix item biases)}
		\FOR{$u = 1$ to $m$}
		\IF{$|\mathcal{I}_u| > 0$}
		\STATE $b_u \leftarrow \frac{\sum_{i \in \mathcal{I}_u} (r_{ui} - \mu - b_i)}{|\mathcal{I}_u| + \lambda}$
		\ENDIF
		\ENDFOR

		\STATE \textit{// Update item biases (fix user biases)}
		\FOR{$i = 1$ to $n$}
		\IF{$|\mathcal{U}_i| > 0$}
		\STATE $b_i \leftarrow \frac{\sum_{u \in \mathcal{U}_i} (r_{ui} - \mu - b_u)}{|\mathcal{U}_i| + \lambda}$
		\ENDIF
		\ENDFOR
		\ENDFOR

		\RETURN $\{b_u\}_{u=1}^m$, $\{b_i\}_{i=1}^n$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
	\caption{Latent Factor ALS with Biases}
	\label{alg:latent-factor-als}
	\begin{algorithmic}[1]
		\REQUIRE Rating matrix $R$, latent factors $k$, regularization $\lambda$, iterations $T$
		\ENSURE User biases $\{b_u\}$, item biases $\{b_i\}$, user factors $P \in \mathbb{R}^{m \times k}$, item factors $Q \in \mathbb{R}^{n \times k}$

		\STATE \textbf{Initialize:}
		\STATE $\mu \leftarrow \frac{1}{|\Omega|} \sum_{(u,i) \in \Omega} r_{ui}$
		\STATE $b_u \leftarrow 0$ for all $u \in [m]$; \quad $b_i \leftarrow 0$ for all $i \in [n]$
		\STATE $P \sim \mathcal{N}(0, 0.1^2 I_{m \times k})$; \quad $Q \sim \mathcal{N}(0, 0.1^2 I_{n \times k})$

		\FOR{$t = 1$ to $T$}
		\STATE \textit{// Update user parameters (fix $Q$ and $\{b_i\}$)}
		\FOR{$u = 1$ to $m$}
		\IF{$|\mathcal{I}_u| > 0$}
		\STATE $Q_u \leftarrow [\mathbf{q}_i]_{i \in \mathcal{I}_u} \in \mathbb{R}^{|\mathcal{I}_u| \times k}$ \COMMENT{Stack item factors}
		\STATE $\tilde{Q}_u \leftarrow [\mathbf{1}_{|\mathcal{I}_u|}, Q_u] \in \mathbb{R}^{|\mathcal{I}_u| \times (k+1)}$ \COMMENT{Augment with bias column}
		\STATE $\mathbf{y}_u \leftarrow [r_{ui} - \mu - b_i]_{i \in \mathcal{I}_u} \in \mathbb{R}^{|\mathcal{I}_u|}$ \COMMENT{Residuals}
		\STATE $A \leftarrow \tilde{Q}_u^\top \tilde{Q}_u + \lambda I_{k+1}$
		\STATE $\mathbf{b} \leftarrow \tilde{Q}_u^\top \mathbf{y}_u$
		\STATE $\mathbf{x}_u \leftarrow A^{-1} \mathbf{b}$ \COMMENT{Solve ridge regression}
		\STATE $b_u \leftarrow x_{u,0}$; \quad $\mathbf{p}_u \leftarrow [x_{u,1}, \ldots, x_{u,k}]^\top$
		\ENDIF
		\ENDFOR

		\STATE \textit{// Update item parameters (fix $P$ and $\{b_u\}$)}
		\FOR{$i = 1$ to $n$}
		\IF{$|\mathcal{U}_i| > 0$}
		\STATE $P_i \leftarrow [\mathbf{p}_u]_{u \in \mathcal{U}_i} \in \mathbb{R}^{|\mathcal{U}_i| \times k}$
		\STATE $\tilde{P}_i \leftarrow [\mathbf{1}_{|\mathcal{U}_i|}, P_i] \in \mathbb{R}^{|\mathcal{U}_i| \times (k+1)}$
		\STATE $\mathbf{y}_i \leftarrow [r_{ui} - \mu - b_u]_{u \in \mathcal{U}_i} \in \mathbb{R}^{|\mathcal{U}_i|}$
		\STATE $A \leftarrow \tilde{P}_i^\top \tilde{P}_i + \lambda I_{k+1}$
		\STATE $\mathbf{b} \leftarrow \tilde{P}_i^\top \mathbf{y}_i$
		\STATE $\mathbf{x}_i \leftarrow A^{-1} \mathbf{b}$
		\STATE $b_i \leftarrow x_{i,0}$; \quad $\mathbf{q}_i \leftarrow [x_{i,1}, \ldots, x_{i,k}]^\top$
		\ENDIF
		\ENDFOR
		\ENDFOR

		\RETURN $\{b_u\}_{u=1}^m$, $\{b_i\}_{i=1}^n$, $P$, $Q$
	\end{algorithmic}
\end{algorithm}

\subsection{Addressing the Cold Start Problem}
We treats cold-start user modeling as a local ALS update problem, solving a small, regularized least-squares system using pre-learned item embeddings. This allows immediate, personalized recommendations with minimal data and negligible computational overhead.


\begin{algorithm}[t]
	\caption{Cold Start Recommendation for New User}
	\label{alg:cold-start}
	\begin{algorithmic}[1]
		\REQUIRE New user ratings $\mathcal{R} = \{(i_j, r_j)\}_{j=1}^{|\mathcal{R}|}$, model parameters $(\mu, Q, \{b_i\}, \lambda)$, top-$K$
		\ENSURE Top-$K$ item recommendations

		\STATE $\mathcal{I} \leftarrow \{i_j : (i_j, r_j) \in \mathcal{R}\}$ \COMMENT{Extract rated item indices}
		\STATE $Q_{\text{new}} \leftarrow [\mathbf{q}_{i_j}]_{i_j \in \mathcal{I}} \in \mathbb{R}^{|\mathcal{R}| \times k}$
		\STATE $\tilde{Q}_{\text{new}} \leftarrow [\mathbf{1}_{|\mathcal{R}|}, Q_{\text{new}}]$
		\STATE $\mathbf{y}_{\text{new}} \leftarrow [r_j - \mu - b_{i_j}]_{j=1}^{|\mathcal{R}|}$

		\STATE \textit{// Compute user parameters via ridge regression}
		\STATE $A \leftarrow \tilde{Q}_{\text{new}}^\top \tilde{Q}_{\text{new}} + \lambda I_{k+1}$
		\STATE $\mathbf{b} \leftarrow \tilde{Q}_{\text{new}}^\top \mathbf{y}_{\text{new}}$
		\STATE $\mathbf{x}_{\text{new}} \leftarrow A^{-1} \mathbf{b}$
		\STATE $b_{\text{new}} \leftarrow x_0$; \quad $\mathbf{p}_{\text{new}} \leftarrow [x_1, \ldots, x_k]^\top$

		\STATE \textit{// Predict ratings for all items}
		\STATE $\mathbf{s} \leftarrow \mu \mathbf{1}_n + b_{\text{new}} \mathbf{1}_n + [b_i]_{i=1}^n + Q \mathbf{p}_{\text{new}}$ \COMMENT{$\mathbf{s} \in \mathbb{R}^n$}
		\STATE $\mathbf{s}[\mathcal{I}] \leftarrow -\infty$ \COMMENT{Exclude already rated items}

		\STATE $\mathcal{T} \leftarrow \text{argsort}(\mathbf{s}, \text{descending})$ \COMMENT{Sort by predicted rating}
		\RETURN $\mathcal{T}[1:K]$, $\mathbf{s}[\mathcal{T}[1:K]]$
	\end{algorithmic}
\end{algorithm}

\subsection{Testing Model}

We test our final model with a dummy user, by liking the movies \emph{Lord of the rings} and \emph{Star wars}. And we have the following top 10 recomendations

\begin{table*}[t]
\centering
\begin{tabular}{cc}
\toprule
Lord of the rings & Star wars \\
\midrule
Christmas Carol: The Movie & Mathu Vadalara
A Creepshow Animated Special & Sioux Ghost Dance
I Start Counting & Christmas Carol: The Movie
Prince and the Showgirl & The Wyvern Mystery
The Wyvern Mystery & Marvel Studios: Expanding the Universe
The Seven Days & 36 China Town
Dave Attell: Captain Miserable & Room in Rome (Habitación en Roma)
Murattu Kalai & Boniface's Holiday
Internet Famous & Bone Tomahawk
Mischief & Sex
\\
\bottomrule
\end{tabular}
\end{table*}


\section{Conclusion}

In this work, we presented an efficient and memory-safe implementation of the Alternating Least Squares (ALS) algorithm for large-scale collaborative filtering. By combining sparse matrix representations (CSR and CSC), a two-pass chunked data ingestion strategy, and careful low-level memory management, our approach enables training on datasets that exceed main memory capacity without resorting to distributed computing frameworks. Experimental results on the MovieLens 100K and 32M datasets demonstrate that the proposed implementation achieves competitive predictive performance, stable convergence, and substantial reductions in memory footprint through the use of float32 precision and vectorized computations.

Beyond scalability, we showed that the learned latent representations exhibit meaningful semantic structure, as evidenced by genre-level clustering in the embedding space. We also addressed the user cold-start problem by formulating new-user personalization as a lightweight, regularized least-squares update that leverages pre-trained item factors, allowing immediate and personalized recommendations with negligible computational overhead.

Overall, this work highlights that careful systems-level design choices can significantly extend the practical applicability of classical matrix factorization methods. Future work includes extending the framework to implicit feedback settings, incorporating temporal dynamics, and exploring hybrid models that integrate side information while preserving the memory efficiency and simplicity of the proposed approach.

\textbf{Note}: All the equations presented in this work are from \cite{ulrich2025}.

All the experiment were carried on a machine with Intel corei3 11th gen processor and 16GB RAM.








% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2023 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% 	\textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% 	\item Submissions must be in PDF\@.
% 	\item \textbf{New to this year}: If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% 	\item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.
% 	\item \textbf{Do not include author information or acknowledgements} in your
% 	      initial submission.
% 	\item Your paper should be in \textbf{10 point Times font}.
% 	\item Make sure your PDF file only uses Type-1 fonts.
% 	\item Place figure captions \emph{under} the figure (and omit titles from inside
% 	      the graphic file itself). Place table captions \emph{over} the table.
% 	\item References must include page numbers whenever possible and be as complete
% 	      as possible. Place multiple citations in chronological order.
% 	\item Do not alter the style template; in particular, do not compress the paper
% 	      format by reducing the vertical spaces.
% 	\item Keep your abstract brief and self-contained, one paragraph and roughly
% 	      4--6 sentences. Gross violations will require correction at the
% 	      camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Paper Deadline:} The deadline for paper submission that is
% advertised on the conference website is strict. If your full,
% anonymized, submission does not reach us on time, it will not be
% considered for publication.

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \textbf{Simultaneous Submission:} ICML will not accept any paper which,
% at the time of submission, is under review for another conference or
% has already been published. This policy also applies to papers that
% overlap substantially in technical content with conference papers
% under review or previously published. ICML submissions must not be
% submitted to other conferences and journals during ICML's review
% period.
% %Authors may submit to ICML substantially different versions of journal papers
% %that are currently under review by the journal, but not yet accepted
% %at the time of submission.
% Informal publications, such as technical
% reports or papers in workshop proceedings which do not appear in
% print, do not fall under these restrictions.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2023}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% 	$\mathit{40}^{th}$ International Conference on Machine Learning},
% Honolulu, Hawaii, USA, PMLR 202, 2023.
% Copyright 2023 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2023\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2023\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2023 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2023.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via CMT\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2023 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% 	should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% 	appear in each column, in the same order as they appear in the text,
% 	but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% 	\vskip 0.2in
% 	\begin{center}
% 		\centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% 		\caption{Historical locations and number of accepted papers for International
% 			Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% 			Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% 			produced, the number of accepted papers for ICML 2008 was unknown and instead
% 			estimated.}
% 		\label{icml-historical}
% 	\end{center}
% 	\vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
% 	\caption{Bubble Sort}
% 	\label{alg:example}
% 	\begin{algorithmic}
% 		\STATE {\bfseries Input:} data $x_i$, size $m$
% 		\REPEAT
% 		\STATE Initialize $noChange = true$.
% 		\FOR{$i=1$ {\bfseries to} $m-1$}
% 		\IF{$x_i > x_{i+1}$}
% 		\STATE Swap $x_i$ and $x_{i+1}$
% 		\STATE $noChange = false$
% 		\ENDIF
% 		\ENDFOR
% 		\UNTIL{$noChange$ is $true$}
% 	\end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% 	\caption{Classification accuracies for naive Bayes and flexible
% 		Bayes on various data sets.}
% 	\label{sample-table}
% 	\vskip 0.15in
% 	\begin{center}
% 		\begin{small}
% 			\begin{sc}
% 				\begin{tabular}{lcccr}
% 					\toprule
% 					Data set  & Naive         & Flexible      & Better?  \\
% 					\midrule
% 					Breast    & 95.9$\pm$ 0.2 & 96.7$\pm$ 0.2 & $\surd$  \\
% 					Cleveland & 83.3$\pm$ 0.6 & 80.0$\pm$ 0.6 & $\times$ \\
% 					Glass2    & 61.9$\pm$ 1.4 & 83.8$\pm$ 0.7 & $\surd$  \\
% 					Credit    & 74.8$\pm$ 0.5 & 78.3$\pm$ 0.6 &          \\
% 					Horse     & 73.3$\pm$ 0.9 & 69.7$\pm$ 1.0 & $\times$ \\
% 					Meta      & 67.1$\pm$ 0.6 & 76.5$\pm$ 0.5 & $\surd$  \\
% 					Pima      & 75.1$\pm$ 0.6 & 73.9$\pm$ 0.5 &          \\
% 					Vehicle   & 44.9$\pm$ 0.6 & 61.5$\pm$ 0.4 & $\surd$  \\
% 					\bottomrule
% 				\end{tabular}
% 			\end{sc}
% 		\end{small}
% 	\end{center}
% 	\vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% 	\label{def:inj}
% 	A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% 	If $f$ is injective mapping a set $X$ to another set $Y$,
% 	the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof}
% 	Left as an exercise to the reader.
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% 	\label{lem:usefullemma}
% 	For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% 	\label{thm:bigtheorem}
% 	If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% 	If $f:X\to Y$ is bijective,
% 	the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% 	The set $X$ is finite.
% 	\label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% 	According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2023.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presenMathu Vadalaratable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the CMT reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one, even using the one-column format.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
